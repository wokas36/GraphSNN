{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import argparse\n",
    "import heapq as hp\n",
    "\n",
    "from graph_data import GraphData\n",
    "from data_reader import DataReader\n",
    "from models import GNN\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--seed'], dest='seed', nargs=None, const=None, default=117, type=<class 'int'>, choices=None, help='Random seed.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "'''\n",
    "----------------------------\n",
    "Dataset  |   batchnorm_dim\n",
    "----------------------------\n",
    "MUTAG    |     28\n",
    "PTC_MR   |     64\n",
    "BZR      |     57\n",
    "COX2     |     56\n",
    "COX2_MD  |     36\n",
    "BZR-MD   |     33\n",
    "PROTEINS |    620\n",
    "D&D      |   5748\n",
    "'''\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--device', default='cpu', help='Select CPU/CUDA for training.')\n",
    "parser.add_argument('--dataset', default='MUTAG', help='Dataset name.')\n",
    "parser.add_argument('--epochs', type=int, default=500, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.009, help='Initial learning rate.') \n",
    "parser.add_argument('--wdecay', type=float, default=9e-3, help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='Batch size.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=64, help='Number of hidden units.')\n",
    "parser.add_argument('--n_layers', type=int, default=2, help='Number of MLP layers for GraphSN.')\n",
    "parser.add_argument('--batchnorm_dim', type=int, default=28, help='Batchnormalization dimension for GraphSN layer.')\n",
    "parser.add_argument('--dropout_1', type=float, default=0.5, help='Dropout rate for concatenation the outputs.') \n",
    "parser.add_argument('--dropout_2', type=float, default=0.6, help='Dropout rate for MLP layers in GraphSN.')\n",
    "parser.add_argument('--n_folds', type=int, default=10, help='Number of folds in cross validation.')\n",
    "parser.add_argument('--threads', type=int, default=0, help='Number of threads.')\n",
    "parser.add_argument('--log_interval', type=int, default=10 , help='Log interval for visualizing outputs.')\n",
    "parser.add_argument('--seed', type=int, default=117, help='Random seed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "making labels sequential, otherwise pytorch might crash\n",
      "N nodes avg/std/min/max: \t17.93/4.58/10/28\n",
      "N edges avg/std/min/max: \t19.79/5.68/10/33\n",
      "Node degree avg/std/min/max: \t2.21/0.74/1/4\n",
      "Node features dim: \t\t7\n",
      "N classes: \t\t\t2\n",
      "Classes: \t\t\t[0 1]\n",
      "Class 0: \t\t\t63 samples\n",
      "Class 1: \t\t\t125 samples\n",
      "feature 0, count 2395/3371\n",
      "feature 1, count 345/3371\n",
      "feature 2, count 593/3371\n",
      "feature 3, count 12/3371\n",
      "feature 4, count 1/3371\n",
      "feature 5, count 23/3371\n",
      "feature 6, count 2/3371\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'fold_files' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b462ef18b098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                         \u001b[0mrnd_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                         \u001b[0mfolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                         use_cont_node_attr=False)\n\u001b[0m",
      "\u001b[1;32mF:\\Projects\\Research\\Task_4\\a_new_perspective_on_how_graph-Supplementary Material\\GraphSNN_Final\\Graph_Classification\\Train_Val\\data_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_dir, fold_dir, rnd_state, use_cont_node_attr, folds)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m#read splits from text file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mtrain_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_ids_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnd_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m#stratified splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'fold_files' referenced before assignment"
     ]
    }
   ],
   "source": [
    "print('Loading data')\n",
    "dataset_fold_idx_path = './data/%s/' % args.dataset.upper() + 'fold_idx/'\n",
    "datareader = DataReader(data_dir='./data/%s/' % args.dataset.upper(),\n",
    "                        fold_dir=dataset_fold_idx_path,\n",
    "                        rnd_state=np.random.RandomState(args.seed),\n",
    "                        folds=args.n_folds,                    \n",
    "                        use_cont_node_attr=False)\n",
    "\n",
    "# datareader = DataReader(data_dir='./data/%s/' % args.dataset.upper(),\n",
    "#                         fold_dir=None,\n",
    "#                         rnd_state=np.random.RandomState(args.seed),\n",
    "#                         folds=args.n_folds,                    \n",
    "#                         use_cont_node_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length = len(datareader.data['adj_list'])\n",
    "for itr in np.arange(dataset_length):\n",
    "    A_array = datareader.data['adj_list'][itr]\n",
    "    G = nx.from_numpy_matrix(A_array)\n",
    "    \n",
    "    sub_graphs = []\n",
    "    subgraph_nodes_list = []\n",
    "    sub_graphs_adj = []\n",
    "    sub_graph_edges = []\n",
    "    new_adj = torch.zeros(A_array.shape[0], A_array.shape[0])\n",
    "\n",
    "    for i in np.arange(len(A_array)):\n",
    "        s_indexes = []\n",
    "        for j in np.arange(len(A_array)):\n",
    "            s_indexes.append(i)\n",
    "            if(A_array[i][j]==1):\n",
    "                s_indexes.append(j)\n",
    "        sub_graphs.append(G.subgraph(s_indexes))\n",
    "\n",
    "    for i in np.arange(len(sub_graphs)):\n",
    "        subgraph_nodes_list.append(list(sub_graphs[i].nodes))\n",
    "        \n",
    "    for index in np.arange(len(sub_graphs)):\n",
    "        sub_graphs_adj.append(nx.adjacency_matrix(sub_graphs[index]).toarray())\n",
    "        \n",
    "    for index in np.arange(len(sub_graphs)):\n",
    "        sub_graph_edges.append(sub_graphs[index].number_of_edges())\n",
    "\n",
    "    for node in np.arange(len(subgraph_nodes_list)):\n",
    "        sub_adj = sub_graphs_adj[node]\n",
    "        for neighbors in np.arange(len(subgraph_nodes_list[node])):\n",
    "            index = subgraph_nodes_list[node][neighbors]\n",
    "            count = torch.tensor(0).float()\n",
    "            if(index==node):\n",
    "                continue\n",
    "            else:\n",
    "                c_neighbors = set(subgraph_nodes_list[node]).intersection(subgraph_nodes_list[index])\n",
    "                if index in c_neighbors:\n",
    "                    nodes_list = subgraph_nodes_list[node]\n",
    "                    sub_graph_index = nodes_list.index(index)\n",
    "                    c_neighbors_list = list(c_neighbors)\n",
    "                    for i, item1 in enumerate(nodes_list):\n",
    "                        if(item1 in c_neighbors):\n",
    "                            for item2 in c_neighbors_list:\n",
    "                                j = nodes_list.index(item2)\n",
    "                                count += sub_adj[i][j]\n",
    "\n",
    "                new_adj[node][index] = count/2\n",
    "                new_adj[node][index] = new_adj[node][index]/(len(c_neighbors)*(len(c_neighbors)-1))\n",
    "                new_adj[node][index] = new_adj[node][index] * (len(c_neighbors)**2)\n",
    "\n",
    "    weight = torch.FloatTensor(new_adj)\n",
    "    weight = weight / weight.sum(1, keepdim=True)\n",
    "    \n",
    "    weight = weight + torch.FloatTensor(A_array)\n",
    "\n",
    "    coeff = weight.sum(1, keepdim=True)\n",
    "    coeff = torch.diag((coeff.T)[0])\n",
    "    \n",
    "    weight = weight + coeff\n",
    "\n",
    "    weight = weight.detach().numpy()\n",
    "    weight = np.nan_to_num(weight, nan=0)\n",
    "\n",
    "    datareader.data['adj_list'][itr] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_folds = []\n",
    "for fold_id in range(args.n_folds):\n",
    "    print('\\nFOLD', fold_id)\n",
    "    loaders = []\n",
    "    for split in ['train', 'test']:\n",
    "        gdata = GraphData(fold_id=fold_id,\n",
    "                             datareader=datareader,\n",
    "                             split=split)\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(gdata, \n",
    "                                             batch_size=args.batch_size,\n",
    "                                             shuffle=split.find('train') >= 0,\n",
    "                                             num_workers=args.threads)\n",
    "        loaders.append(loader)\n",
    "    \n",
    "    model = GNN(input_dim=loaders[0].dataset.features_dim,\n",
    "                hidden_dim=args.hidden_dim,\n",
    "                output_dim=loaders[0].dataset.n_classes,\n",
    "                n_layers=args.n_layers,\n",
    "                batchnorm_dim=args.batchnorm_dim, \n",
    "                dropout_1=args.dropout_1, \n",
    "                dropout_2=args.dropout_2).to(args.device)\n",
    "\n",
    "    print('\\nInitialize model')\n",
    "    print(model)\n",
    "    c = 0\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        c += p.numel()\n",
    "    print('N trainable parameters:', c)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=args.lr,\n",
    "                weight_decay=args.wdecay,\n",
    "                betas=(0.5, 0.999))\n",
    "    \n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.5)\n",
    "\n",
    "    def train(train_loader):\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        train_loss, n_samples = 0, 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            time_iter = time.time() - start\n",
    "            train_loss += loss.item() * len(output)\n",
    "            n_samples += len(output)\n",
    "            if batch_idx % args.log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} (avg: {:.6f}) \\tsec/iter: {:.4f}'.format(\n",
    "                    epoch, n_samples, len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples, time_iter / (batch_idx + 1) ))\n",
    "\n",
    "    def test(test_loader):\n",
    "        model.eval()\n",
    "        start = time.time()\n",
    "        test_loss, correct, n_samples = 0, 0, 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(args.device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, data[4], reduction='sum')\n",
    "            test_loss += loss.item()\n",
    "            n_samples += len(output)\n",
    "            pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "\n",
    "            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "\n",
    "        time_iter = time.time() - start\n",
    "\n",
    "        test_loss /= n_samples\n",
    "\n",
    "        acc = 100. * correct / n_samples\n",
    "        print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch, \n",
    "                                                                                              test_loss, \n",
    "                                                                                              correct, \n",
    "                                                                                              n_samples, acc))\n",
    "        return acc\n",
    "\n",
    "    loss_fn = F.cross_entropy\n",
    "    max_acc = 0.0\n",
    "    for epoch in range(args.epochs):\n",
    "        train(loaders[0])\n",
    "        acc = test(loaders[1])\n",
    "        max_acc = max(max_acc, acc)\n",
    "    acc_folds.append(max_acc)\n",
    "\n",
    "print(acc_folds)\n",
    "print('{}-fold cross validation avg acc (+- std): {} ({})'.format(args.n_folds, np.mean(acc_folds), np.std(acc_folds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
